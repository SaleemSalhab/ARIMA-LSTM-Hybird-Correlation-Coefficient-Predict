{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b9de6549",
   "metadata": {},
   "source": [
    "# ARIMA-LSTM MODEL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e986eaa",
   "metadata": {},
   "source": [
    "## In this notebook, we'll put all codes we use to build this model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5ba67b0",
   "metadata": {},
   "source": [
    "# 1. ARIMA MODEL SECTION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5a7455d",
   "metadata": {},
   "source": [
    "#### Web scraping code to get data we need"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dcc44de",
   "metadata": {},
   "source": [
    "## S&P500 Item List"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "344ec3e9",
   "metadata": {},
   "source": [
    "First, the universe of our research needs to be set. I decided to use the S&P500 since it comprises old as well as fairly young and big companies. I will be deriving a sample portfolio among these 505 companies enlisted in the S&P500 firms to elaborate on my thesis. From wikipedia, I crawled the s&p500 company list along with its tickers and its industry domain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aee2e89",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import urllib.request\n",
    "import urllib.parse\n",
    "import requests\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "#Get total list of S&P500 companies\n",
    "url='https://en.wikipedia.org/wiki/List_of_S%26P_500_companies'\n",
    "with urllib.request.urlopen(url) as response:\n",
    "    html = response.read()\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    table = soup.find('table',{'class':'wikitable sortable'})\n",
    "    tr_list = table.find_all(\"tr\")\n",
    "    \n",
    "    ticker = []\n",
    "    company = []\n",
    "    GICS_sector = []\n",
    "    GICS_sub_industry = []\n",
    "    for unit in tr_list[1:] :  #excluded the first 'tr' which refers to variable names of the table\n",
    "        td_list = unit.find_all(\"td\")\n",
    "        ticker.append(td_list[0].text)\n",
    "        company.append(td_list[1].text)\n",
    "        GICS_sector.append(td_list[3].text)\n",
    "        GICS_sub_industry.append(td_list[4].text)\n",
    "    SP500 = {'ticker':ticker, 'company':company, 'GICS_sector':GICS_sector, 'GICS_sub_industry':GICS_sub_industry}\n",
    "    SP_df = pd.DataFrame(SP500)\n",
    "    print(SP_df)\n",
    "    \n",
    "    SP_df.to_csv(\"C:/Users/Froilan/Desktop/myFiles/JupyterFiles/stock_correlation_prediction/SP500_list.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9e525a8",
   "metadata": {},
   "source": [
    "## S&P500 Price Data\n",
    "\n",
    "Using the scraped list of S&P500 firms, I downloaded the price data for each firms into 505 csv files with the Quandl api."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "658c42f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import quandl\n",
    "import os\n",
    "\n",
    "API_KEY = ''\n",
    "start = \"2000-01-01\"\n",
    "end = \"2017-12-31\"\n",
    "tickers = list(SP_df.ticker)\n",
    "print(tickers)\n",
    "\n",
    "path = 'C:/Users/Froilan/Desktop/myFiles/JupyterFiles/stock_correlation_prediction/stock_price_data'\n",
    "for file in os.listdir(path) :\n",
    "    os.remove(path+'/'+file)\n",
    "for item in tickers :\n",
    "    data = quandl.get(\"WIKI/{}\".format(item.replace(\".\",\"_\")), start_date=start, end_date=end, api_key=API_KEY)     \n",
    "    data_dir = \"C:/Users/Froilan/Desktop/myFiles/JupyterFiles/stock_correlation_prediction/stock_price_data/\"+item+\".csv\"\n",
    "    data.to_csv(data_dir)\n",
    "    \n",
    "    if os.path.getsize(data_dir) < 250000 :\n",
    "        print(item+' file size '+str(os.path.getsize(data_dir))+' bytes : reloading data...')\n",
    "        data = quandl.get(\"WIKI/{}\".format(item.replace(\".\",\"_\")), start_date=start, end_date=end, api_key=API_KEY)   \n",
    "        data.to_csv(data_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0db46986",
   "metadata": {},
   "source": [
    "## Data Preprocessing\n",
    "\n",
    "Data preprocessing codes such as NA imputation, reshaping etc.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af229258",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import math\n",
    "import os\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddd3ab6d",
   "metadata": {},
   "source": [
    "### Item Selection\n",
    "\n",
    "I selected all the assets that had data from 2008-01-01 among the S&P500 stock list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f86de61c",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'C:/Users/Froilan/Desktop/myFiles/JupyterFiles/stock_correlation_prediction/stock_data'\n",
    "stock08 = []\n",
    "for file in os.listdir(path):\n",
    "    file_path = path + '/' + file\n",
    "    date = pd.read_csv(file_path)['Date']\n",
    "    if len(date)>0 and pd.read_csv(file_path)['Date'][0] <= '2008-01-01' :\n",
    "        stock08.append(file)\n",
    "print(str(len(stock08))+\" stocks selected\")\n",
    "print(stock08)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dd2cab9",
   "metadata": {},
   "source": [
    "### Organize Data\n",
    "\n",
    "In order to keep concise and deal with missing data, I concatenated all the price data of the selected items above to a single dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d442450",
   "metadata": {},
   "outputs": [],
   "source": [
    "stock_price_dict = {}\n",
    "\n",
    "for file in stock08 :\n",
    "    path = \"C:/Users/Froilan/Desktop/myFiles/JupyterFiles/stock_correlation_prediction/stock_data/\" + file\n",
    "    df = pd.read_csv(path)\n",
    "    df = df[df.Date >= '2008-01-01']\n",
    "    pd.to_datetime(df['Date'], format='%Y-%m-%d')\n",
    "    df = df.set_index(pd.DatetimeIndex(df['Date']))\n",
    "    stock_price_dict[file.split(\".\")[0]] = df['Adj. Close']\n",
    "\n",
    "market_path = \"C:/Users/Froilan/Desktop/myFiles/JupyterFiles/stock_correlation_prediction/SP500_index.csv\"\n",
    "df = pd.read_csv(market_path)\n",
    "pd.to_datetime(df['Date'], format='%Y-%m-%d')\n",
    "df = df.set_index(pd.DatetimeIndex(df['Date']))\n",
    "stock_price_dict['SP500'] = df['Adj Close']\n",
    "    \n",
    "stock_price_df = pd.DataFrame(stock_price_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02ef31d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(stock_price_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af59dd8d",
   "metadata": {},
   "source": [
    "### Dealing with Missing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1a4331e",
   "metadata": {},
   "outputs": [],
   "source": [
    "NA_col = []\n",
    "NA_ratio = []\n",
    "for col in stock_price_df.columns :\n",
    "    na_index = np.where(stock_price_df[col].isnull())[0]\n",
    "    NA_col.append(col)\n",
    "    NA_ratio.append(len(na_index)/stock_price_df.shape[0] * 100)\n",
    "    print(col,na_index)\n",
    "NA_df = pd.DataFrame({'tickers':NA_col,'NA_ratio':NA_ratio})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce86e9b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "NA_df.plot.bar(rot=0, figsize=(18,4))\n",
    "plt.tick_params(axis='x', which='both', bottom=True, top=False, labelbottom=False)\n",
    "plt.xlabel('tickers')\n",
    "plt.ylabel('NA ratio (%)')\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dee3626a",
   "metadata": {},
   "source": [
    "Most of the dataset that has missing data has only one or two data points missing. It would be rational enough to impute the data points with the data from the day right before.\n",
    "\n",
    "However, one company has quite some missing data. 'MMM' is the only company that has high proportion of missing data. Leaving out this one company from the S&P 500 firms wouldn't be a big issue. So I'll drop the 'MMM' column the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfee608a",
   "metadata": {},
   "outputs": [],
   "source": [
    "stock_price_df = stock_price_df.drop(['MMM'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97cf15ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def impute_data(column_name):\n",
    "    index = stock_price_df.index.values[0]\n",
    "    price_na_index = np.where(stock_price_df[column_name].isnull())[0]\n",
    "    for i in price_na_index :\n",
    "        stock_price_df[column_name][i] = stock_price_df[column_name][i-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1602271",
   "metadata": {},
   "outputs": [],
   "source": [
    "for item in stock_price_df.columns :\n",
    "    impute_data(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55504f30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final Check for NaN\n",
    "for item in stock_price_df.columns :\n",
    "    if stock_price_df[item].isnull().values.any() :\n",
    "        print('stock price data of '+item+' still has NaN')\n",
    "print(\"END OF CHECKING. NO NA REMAINING\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fb743d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "stock_price_df.to_csv(\"C:/Users/Froilan/Desktop/myFiles/JupyterFiles/stock_correlation_prediction/stock08_price.csv\",index_label='Date')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ccaae91",
   "metadata": {},
   "source": [
    "### Create Portfolio\n",
    "\n",
    "out of 505 companies, 150 firms are randomly selected for the portfolio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04c62c03",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"C:/Users/Froilan/Desktop/myFiles/JupyterFiles/stock_correlation_prediction/stock08_price.csv\")\n",
    "universe = list(df.columns.values[1:])\n",
    "universe.remove(\"SP500\")\n",
    "print(universe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69d1266b",
   "metadata": {},
   "outputs": [],
   "source": [
    "random.shuffle(universe)\n",
    "portfolio = universe[:150].copy()\n",
    "\n",
    "print(portfolio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49ae27e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#FOR LIST REUSE#FOR LIS \n",
    "portfolio = ['CELG', 'PXD', 'WAT', 'LH', 'AMGN', 'AOS', 'EFX', 'CRM', 'NEM', 'JNPR', 'LB', 'CTAS', 'MAT', 'MDLZ', 'VLO', 'APH', 'ADM', 'MLM', 'BK', 'NOV', 'BDX', 'RRC', 'IVZ', 'ED', 'SBUX', 'GRMN', 'CI', 'ZION', 'COO', 'TIF', 'RHT', 'FDX', 'LLL', 'GLW', 'GPN', 'IPGP', 'GPC', 'HPQ', 'ADI', 'AMG', 'MTB', 'YUM', 'SYK', 'KMX', 'AME', 'AAP', 'DAL', 'A', 'MON', 'BRK', 'BMY', 'KMB', 'JPM', 'CCI', 'AET', 'DLTR', 'MGM', 'FL', 'HD', 'CLX', 'OKE', 'UPS', 'WMB', 'IFF', 'CMS', 'ARNC', 'VIAB', 'MMC', 'REG', 'ES', 'ITW', 'NDAQ', 'AIZ', 'VRTX', 'CTL', 'QCOM', 'MSI', 'NKTR', 'AMAT', 'BWA', 'ESRX', 'TXT', 'EXR', 'VNO', 'BBT', 'WDC', 'UAL', 'PVH', 'NOC', 'PCAR', 'NSC', 'UAA', 'FFIV', 'PHM', 'LUV', 'HUM', 'SPG', 'SJM', 'ABT', 'CMG', 'ALK', 'ULTA', 'TMK', 'TAP', 'SCG', 'CAT', 'TMO', 'AES', 'MRK', 'RMD', 'MKC', 'WU', 'ACN', 'HIG', 'TEL', 'DE', 'ATVI', 'O', 'UNM', 'VMC', 'ETFC', 'CMA', 'NRG', 'RHI', 'RE', 'FMC', 'MU', 'CB', 'LNT', 'GE', 'CBS', 'ALGN', 'SNA', 'LLY', 'LEN', 'MAA', 'OMC', 'F', 'APA', 'CDNS', 'SLG', 'HP', 'XLNX', 'SHW', 'AFL', 'STT', 'PAYX', 'AIG', 'FOX', 'MA']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76422ab6",
   "metadata": {},
   "source": [
    "### Prepare the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7b5753a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rolling_corr(item1,item2) :\n",
    "    #import data\n",
    "    stock_price_df = pd.read_csv(\"C:/Users/Froilan/Desktop/myFiles/JupyterFiles/stock_correlation_prediction/stock08_price.csv\")\n",
    "    pd.to_datetime(stock_price_df['Date'], format='%Y-%m-%d')\n",
    "    stock_price_df = stock_price_df.set_index(pd.DatetimeIndex(stock_price_df['Date']))\n",
    "    \n",
    "    #calculate\n",
    "    df_pair = pd.concat([stock_price_df[item1], stock_price_df[item2]], axis=1)\n",
    "    df_pair.columns = [item1,item2]\n",
    "    df_corr = df_pair[item1].rolling(window=100).corr(df_pair[item2])\n",
    "    return df_corr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a44d424",
   "metadata": {},
   "outputs": [],
   "source": [
    "index_list = []\n",
    "for _ in range(100):\n",
    "    indices = []\n",
    "    for k in range(_, 2420,100):\n",
    "        indices.append(k)\n",
    "    index_list.append(indices)\n",
    "    \n",
    "data_matrix = []\n",
    "count = 0\n",
    "for i in range(150):\n",
    "    for j in range(149-i):\n",
    "        a = portfolio[i]\n",
    "        b = portfolio[149-j]\n",
    "        file_name = a + '_' + b\n",
    "            \n",
    "        corr_series = rolling_corr(a, b)[99:]\n",
    "        for _ in range(100):\n",
    "            corr_strided = list(corr_series[index_list[_]][:24]).copy()\n",
    "            data_matrix.append(corr_strided)\n",
    "            count+=1\n",
    "            if count % 1000 == 0 :\n",
    "                print(str(count)+' items preprocessed')\n",
    "                \n",
    "data_matrix = np.transpose(data_matrix)\n",
    "data_dictionary = {}\n",
    "for i in range(len(data_matrix)):\n",
    "    data_dictionary[str(i)] = data_matrix[i]\n",
    "data_df = pd.DataFrame(data_dictionary)\n",
    "data_df.to_csv('C:/Users/Froilan/Desktop/myFiles/JupyterFiles/stock_correlation_prediction/dataset.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9465d925",
   "metadata": {},
   "source": [
    "# ARIMA MODELING\n",
    "\n",
    "The ARIMA codes to compute the residual values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a597156e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import scipy.stats as stats\n",
    "import pylab as pl\n",
    "from pyramid.arima import ARIMA, auto_arima\n",
    "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
    "import warnings\n",
    "warnings.simplefilter(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f7d87a3",
   "metadata": {},
   "source": [
    "## Data Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d681613b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df = pd.read_csv('C:/Users/Froilan/Desktop/myFiles/JupyterFiles/stock_correlation_prediction/dataset.csv')\n",
    "data_df = data_df.loc[:, ~data_df.columns.str.contains('^Unnamed')]\n",
    "print(data_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98181372",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_list = []\n",
    "for i in range(24):\n",
    "    num_list.append(str(i))\n",
    "data_df = data_df[num_list].copy()\n",
    "data_df = np.transpose(data_df)\n",
    "print(data_df.shape)\n",
    "print(data_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "441fa3ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d60a029",
   "metadata": {},
   "source": [
    "## Train-Dev-Test Split\n",
    "\n",
    "We do not split X and Y yet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "498aa7b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = [20*k for k in range(55875)]\n",
    "data_df = pd.DataFrame(data_df[indices])\n",
    "\n",
    "train = []\n",
    "dev = []\n",
    "test1 = []\n",
    "test2 = []\n",
    "\n",
    "for i in range(data_df.shape[1]):\n",
    "    tmp = data_df[20*i].copy()\n",
    "    train.append(tmp[:21])\n",
    "    dev.append(tmp[1:22])\n",
    "    test1.append(tmp[2:23])\n",
    "    test2.append(tmp[3:24])\n",
    "    \n",
    "train = pd.DataFrame(train)\n",
    "dev = pd.DataFrame(dev)\n",
    "test1 = pd.DataFrame(test1)\n",
    "test2 = pd.DataFrame(test2)\n",
    "\n",
    "train.to_csv('C:/Users/Froilan/Desktop/myFiles/JupyterFiles/stock_correlation_prediction/train_dev_test/before_arima/train.csv')\n",
    "dev.to_csv('C:/Users/Froilan/Desktop/myFiles/JupyterFiles/stock_correlation_prediction/train_dev_test/before_arima/dev.csv')\n",
    "test1.to_csv('C:/Users/Froilan/Desktop/myFiles/JupyterFiles/stock_correlation_prediction/train_dev_test/before_arima/test1.csv')\n",
    "test2.to_csv('C:/Users/Froilan/Desktop/myFiles/JupyterFiles/stock_correlation_prediction/train_dev_test/before_arima/test2.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d3a178f",
   "metadata": {},
   "source": [
    "## EDA for ARIMA modeling\n",
    "\n",
    "### Plotting the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c130197",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('C:/Users/Froilan/Desktop/myFiles/JupyterFiles/stock_correlation_prediction/train_dev_test/before_arima/train.csv')\n",
    "train = np.transpose(train.loc[:, ~train.columns.str.contains('^Unnamed')])\n",
    "for _ in range(100):\n",
    "    randint = random.randrange(0,55875,1)\n",
    "    print(randint)\n",
    "    train[randint].plot()\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "    plot_acf(train[randint].diff()[1:])\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "    plot_pacf(train[randint].diff()[1:])\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "    print('----------------------------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2687672",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean = sorted(np.array(stat.iloc[1,:].copy()))\n",
    "stdev = sorted(np.array(stat.iloc[2,:].copy()))\n",
    "fit1 = stats.norm.pdf(mean, np.mean(mean), np.std(mean))\n",
    "fit2 = stats.norm.pdf(stdev, np.mean(stdev), np.std(stdev))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "549be0c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "pl.plot(mean,fit1,color='blue')\n",
    "pl.hist(mean,normed=True,color='grey')\n",
    "pl.title('time series mean histogram')\n",
    "pl.xlabel('mean')\n",
    "pl.show()\n",
    "pl.close()\n",
    "pl.plot(stdev,fit2,color='blue')\n",
    "pl.hist(stdev,normed=True,color='grey')\n",
    "pl.title('time series standard deviation histogram')\n",
    "pl.xlabel('standard deviation')\n",
    "pl.show()\n",
    "pl.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e286ba9f",
   "metadata": {},
   "source": [
    "## ARIMA Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d45835cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('C:/Users/Froilan/Desktop/myFiles/JupyterFiles/stock_correlation_prediction/train_dev_test/before_arima/train.csv')\n",
    "dev = pd.read_csv('C:/Users/Froilan/Desktop/myFiles/JupyterFiles/stock_correlation_prediction/train_dev_test/before_arima/dev.csv')\n",
    "test1 = pd.read_csv('C:/Users/Froilan/Desktop/myFiles/JupyterFiles/stock_correlation_prediction/train_dev_test/before_arima/test1.csv')\n",
    "test2 = pd.read_csv('C:/Users/Froilan/Desktop/myFiles/JupyterFiles/stock_correlation_prediction/train_dev_test/before_arima/test2.csv')\n",
    "\n",
    "train = np.transpose(train.loc[:,~train.columns.str.contains('^Unnamed')])\n",
    "dev = np.transpose(dev.loc[:,~dev.columns.str.contains('^Unnamed')])\n",
    "test1 = np.transpose(test1.loc[:,~test1.columns.str.contains('^Unnamed')])\n",
    "test2 = np.transpose(test2.loc[:,~test2.columns.str.contains('^Unnamed')])\n",
    "\n",
    "datasets = [train, dev, test1, test2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be83b292",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_110 = ARIMA(order=(1,1,0), method='mle', suppress_warnings=True)\n",
    "model_011 = ARIMA(order=(0,1,1), method='mle', suppress_warnings=True)\n",
    "model_111 = ARIMA(order=(1,1,1), method='mle', suppress_warnings=True)\n",
    "model_211 = ARIMA(order=(2,1,1), method='mle', suppress_warnings=True)\n",
    "model_210 = ARIMA(order=(2,1,0), method='mle', suppress_warnings=True)\n",
    "\n",
    "train_X = []; train_Y = []\n",
    "dev_X = []; dev_Y = []\n",
    "test1_X = []; test1_Y = []\n",
    "test2_X = []; test2_Y = []\n",
    "\n",
    "flag = 0\n",
    "\n",
    "for i in range(55875):\n",
    "    print(i)\n",
    "    tmp = []\n",
    "    c=0\n",
    "    for s in datasets :\n",
    "        c+=1\n",
    "        try:\n",
    "            model1 = model_110.fit(s[i])\n",
    "            model = model1\n",
    "            \n",
    "            try:\n",
    "                model2 = model_011.fit(s[i])\n",
    "                \n",
    "                if model.aic() <= model2.aic() :\n",
    "                    pass\n",
    "                else :\n",
    "                    model = model2\n",
    "                    \n",
    "                try :\n",
    "                    model3 = model_111.fit(s[i])\n",
    "                    if model.aic() <= model3.aic() :\n",
    "                        pass\n",
    "                    else :\n",
    "                        model = model3\n",
    "                except :\n",
    "                    try:\n",
    "                        model4 = model_211.fit(s[i])\n",
    "                        \n",
    "                        if model.aic() <= model4.aic() :\n",
    "                            pass\n",
    "                        else:\n",
    "                            model = model4\n",
    "                    except:\n",
    "                        try:\n",
    "                            model5 = model_210.fit(s[i])\n",
    "                            \n",
    "                            if model.aic() <= model5.aic():\n",
    "                                pass\n",
    "                            else :\n",
    "                                model = model5\n",
    "                        except :\n",
    "                            pass\n",
    "                    \n",
    "            except:\n",
    "                try:\n",
    "                    model3 = model_111.fit(s[i])\n",
    "\n",
    "                    if model.aic() <= model3.aic() :\n",
    "                        pass\n",
    "                    else :\n",
    "                        model = model3\n",
    "                except :\n",
    "                    try:\n",
    "                        model4 = model_211.fit(s[i])\n",
    "                        \n",
    "                        if model.aic() <= model4.aic() :\n",
    "                            pass\n",
    "                        else:\n",
    "                            model = model4\n",
    "                    except:\n",
    "                        try:\n",
    "                            model5 = model_210.fit(s[i])\n",
    "                            \n",
    "                            if model.aic() <= model5.aic():\n",
    "                                pass\n",
    "                            else :\n",
    "                                model = model5\n",
    "                        except :\n",
    "                            pass\n",
    "                \n",
    "        except:\n",
    "            try:\n",
    "                model2 = model_011.fit(s[i])\n",
    "                model = model2\n",
    "            \n",
    "                try :\n",
    "                    model3 = model_111.fit(s[i])\n",
    "                    \n",
    "                    if model.aic() <= model3.aic():\n",
    "                        pass\n",
    "                    else:\n",
    "                        model = model3\n",
    "                except :\n",
    "                    try:\n",
    "                        model4 = model_211.fit(s[i])\n",
    "                        \n",
    "                        if model.aic() <= model4.aic() :\n",
    "                            pass\n",
    "                        else:\n",
    "                            model = model4\n",
    "                    except:\n",
    "                        try:\n",
    "                            model5 = model_210.fit(s[i])\n",
    "                            \n",
    "                            if model.aic() <= model5.aic():\n",
    "                                pass\n",
    "                            else :\n",
    "                                model = model5\n",
    "                        except :\n",
    "                            pass\n",
    "            \n",
    "            except :\n",
    "                try:\n",
    "                    model3 = model_111.fit(s[i])\n",
    "                    model = model3\n",
    "                except :\n",
    "                    try:\n",
    "                        model4 = model_211.fit(s[i])\n",
    "                        \n",
    "                        if model.aic() <= model4.aic() :\n",
    "                            pass\n",
    "                        else:\n",
    "                            model = model4\n",
    "                    except:\n",
    "                        try:\n",
    "                            model5 = model_210.fit(s[i])\n",
    "                            \n",
    "                            if model.aic() <= model5.aic():\n",
    "                                pass\n",
    "                            else :\n",
    "                                model = model5\n",
    "                        except :\n",
    "                            flag = 1\n",
    "                            print(str(c) + \" FATAL ERROR\")\n",
    "                            break\n",
    "        \n",
    "        predictions = list(model.predict_in_sample())\n",
    "        #pad the first time step of predictions with the average of the prediction values\n",
    "        #so as to match the length of the s[i] data\n",
    "        predictions = [np.mean(predictions)] + predictions\n",
    "        \n",
    "        residual = pd.Series(np.array(s[i]) - np.array(predictions))\n",
    "        tmp.append(np.array(residual))\n",
    "        \n",
    "                    \n",
    "    if flag == 1:\n",
    "        break\n",
    "    train_X.append(tmp[0][:20])\n",
    "    train_Y.append(tmp[0][20])\n",
    "    dev_X.append(tmp[1][:20])\n",
    "    dev_Y.append(tmp[1][20])\n",
    "    test1_X.append(tmp[2][:20])\n",
    "    test1_Y.append(tmp[2][20])\n",
    "    test2_X.append(tmp[3][:20])\n",
    "    test2_Y.append(tmp[3][20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80304e1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(train_X).to_csv('C:/Users/Froilan/Desktop/myFiles/JupyterFiles/stock_correlation_prediction/train_dev_test/after_arima/train_X.csv')\n",
    "pd.DataFrame(dev_X).to_csv('C:/Users/Froilan/Desktop/myFiles/JupyterFiles/stock_correlation_prediction/train_dev_test/after_arima/dev_X.csv')\n",
    "pd.DataFrame(test1_X).to_csv('C:/Users/Froilan/Desktop/myFiles/JupyterFiles/stock_correlation_prediction/train_dev_test/after_arima/test1_X.csv')\n",
    "pd.DataFrame(test2_X).to_csv('C:/Users/Froilan/Desktop/myFiles/JupyterFiles/stock_correlation_prediction/train_dev_test/after_arima/test2_X.csv')\n",
    "pd.DataFrame(train_Y).to_csv('C:/Users/Froilan/Desktop/myFiles/JupyterFiles/stock_correlation_prediction/train_dev_test/after_arima/train_Y.csv')\n",
    "pd.DataFrame(dev_Y).to_csv('C:/Users/Froilan/Desktop/myFiles/JupyterFiles/stock_correlation_prediction/train_dev_test/after_arima/dev_Y.csv')\n",
    "pd.DataFrame(test1_Y).to_csv('C:/Users/Froilan/Desktop/myFiles/JupyterFiles/stock_correlation_prediction/train_dev_test/after_arima/test1_Y.csv')\n",
    "pd.DataFrame(test2_Y).to_csv('C:/Users/Froilan/Desktop/myFiles/JupyterFiles/stock_correlation_prediction/train_dev_test/after_arima/test2_Y.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17fe3b67",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('C:/Users/Froilan/Desktop/myFiles/JupyterFiles/stock_correlation_prediction/train_dev_test/after_arima/train_X.csv')\n",
    "train = np.transpose(train.loc[:,~train.columns.str.contains('^Unnamed')])\n",
    "train_melt = sorted(np.array(train.melt()['value']))\n",
    "fit = stats.norm.pdf(train_melt, np.mean(train_melt), np.std(train_melt))\n",
    "pl.hist(train_melt,normed=True, color='grey', bins=[-4,-3,-2,-1,0,1,2,3,4,5])\n",
    "pl.plot(train_melt,fit,color='blue')\n",
    "pl.title('residual value distribution')\n",
    "pl.xlabel('residual')\n",
    "pl.show()\n",
    "pl.close()\n",
    "\n",
    "X = [x for x in train_melt if x>2]\n",
    "Y = [y for y in train_melt if y<-2]\n",
    "out_of_bound = X + Y\n",
    "print(str(len(out_of_bound)/11175) +' % of the data is out of bound [-2,2]')\n",
    "\n",
    "X = [x for x in train_melt if x>1]\n",
    "Y = [y for y in train_melt if y<-1]\n",
    "out_of_bound = X + Y\n",
    "print(str(len(out_of_bound)/11175) +' % of the data is out of bound [-1,1]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb3fb299",
   "metadata": {},
   "outputs": [],
   "source": [
    "stat = pd.DataFrame()\n",
    "for i in range(55875):\n",
    "    df = train[i].describe()\n",
    "    stat[i] = df\n",
    "stat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63bb621e",
   "metadata": {},
   "source": [
    "## NEW ASSET ARIMA MODELING\n",
    "\n",
    "After generating model, we test on different assets iteratively. This is the ARIMA section of it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02a37dcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from pyramid.arima import ARIMA, auto_arima\n",
    "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
    "import warnings\n",
    "warnings.simplefilter(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b6ad6d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv('C:/Users/Froilan/Desktop/myFiles/JupyterFiles/stock_correlation_prediction/train_dev_test/new_asset_before_arima.csv')\n",
    "dataset = dataset.loc[:,~dataset.columns.str.contains('Unnamed')]\n",
    "\n",
    "model_110 = ARIMA(order=(1,1,0), method='mle', suppress_warnings=True)\n",
    "model_011 = ARIMA(order=(0,1,1), method='mle', suppress_warnings=True)\n",
    "model_111 = ARIMA(order=(1,1,1), method='mle', suppress_warnings=True)\n",
    "model_211 = ARIMA(order=(2,1,1), method='mle', suppress_warnings=True)\n",
    "model_210 = ARIMA(order=(2,1,0), method='mle', suppress_warnings=True)\n",
    "\n",
    "flag = 0\n",
    "c=0\n",
    "residual = []\n",
    "for s in np.array(dataset):\n",
    "    c+=1\n",
    "    try:\n",
    "        model1 = model_110.fit(s)\n",
    "        model = model1\n",
    "\n",
    "        try:\n",
    "            model2 = model_011.fit(s)\n",
    "\n",
    "            if model.aic() <= model2.aic() :\n",
    "                pass\n",
    "            else :\n",
    "                model = model2\n",
    "\n",
    "            try :\n",
    "                model3 = model_111.fit(s)\n",
    "                if model.aic() <= model3.aic() :\n",
    "                    pass\n",
    "                else :\n",
    "                    model = model3\n",
    "            except :\n",
    "                try:\n",
    "                    model4 = model_211.fit(s)\n",
    "\n",
    "                    if model.aic() <= model4.aic() :\n",
    "                        pass\n",
    "                    else:\n",
    "                        model = model4\n",
    "                except:\n",
    "                    try:\n",
    "                        model5 = model_210.fit(s)\n",
    "\n",
    "                        if model.aic() <= model5.aic():\n",
    "                            pass\n",
    "                        else :\n",
    "                            model = model5\n",
    "                    except :\n",
    "                        pass\n",
    "\n",
    "        except:\n",
    "            try:\n",
    "                model3 = model_111.fit(s)\n",
    "\n",
    "                if model.aic() <= model3.aic() :\n",
    "                    pass\n",
    "                else :\n",
    "                    model = model3\n",
    "            except :\n",
    "                try:\n",
    "                    model4 = model_211.fit(s)\n",
    "\n",
    "                    if model.aic() <= model4.aic() :\n",
    "                        pass\n",
    "                    else:\n",
    "                        model = model4\n",
    "                except:\n",
    "                    try:\n",
    "                        model5 = model_210.fit(s)\n",
    "\n",
    "                        if model.aic() <= model5.aic():\n",
    "                            pass\n",
    "                        else :\n",
    "                            model = model5\n",
    "                    except :\n",
    "                        pass\n",
    "\n",
    "    except:\n",
    "        try:\n",
    "            model2 = model_011.fit(s[i])\n",
    "            model = model2\n",
    "\n",
    "            try :\n",
    "                model3 = model_111.fit(s[i])\n",
    "\n",
    "                if model.aic() <= model3.aic():\n",
    "                    pass\n",
    "                else:\n",
    "                    model = model3\n",
    "            except :\n",
    "                try:\n",
    "                    model4 = model_211.fit(s[i])\n",
    "\n",
    "                    if model.aic() <= model4.aic() :\n",
    "                        pass\n",
    "                    else:\n",
    "                        model = model4\n",
    "                except:\n",
    "                    try:\n",
    "                        model5 = model_210.fit(s[i])\n",
    "\n",
    "                        if model.aic() <= model5.aic():\n",
    "                            pass\n",
    "                        else :\n",
    "                            model = model5\n",
    "                    except :\n",
    "                        pass\n",
    "\n",
    "        except :\n",
    "            try:\n",
    "                model3 = model_111.fit(s[i])\n",
    "                model = model3\n",
    "            except :\n",
    "                try:\n",
    "                    model4 = model_211.fit(s[i])\n",
    "\n",
    "                    if model.aic() <= model4.aic() :\n",
    "                        pass\n",
    "                    else:\n",
    "                        model = model4\n",
    "                except:\n",
    "                    try:\n",
    "                        model5 = model_210.fit(s[i])\n",
    "\n",
    "                        if model.aic() <= model5.aic():\n",
    "                            pass\n",
    "                        else :\n",
    "                            model = model5\n",
    "                    except :\n",
    "                        flag = 1\n",
    "                        print(str(c) + \" FATAL ERROR\")\n",
    "                        break\n",
    "\n",
    "                        \n",
    "    predictions = list(model.predict_in_sample())\n",
    "\n",
    "    predictions = [np.mean(predictions)] + predictions\n",
    "\n",
    "    res = pd.Series(np.array(s) - np.array(predictions))\n",
    "    residual.append(np.array(res))\n",
    "\n",
    "    if flag == 1:\n",
    "        break\n",
    "residual = pd.DataFrame(residual)\n",
    "residual.to_csv('C:/Users/Froilan/Desktop/myFiles/JupyterFiles/stock_correlation_prediction/train_dev_test/new_asset_after_arima.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aad3d38",
   "metadata": {},
   "source": [
    "# 2.  LSTM-CELL RNN MODEL SECTION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c7ae52d",
   "metadata": {},
   "source": [
    "## raw pytho codes\n",
    "\n",
    "The python codes used to model the LSTM RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6965d3f1",
   "metadata": {},
   "source": [
    "### new_asset_testing_afterARIMA.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f151e4ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import Dense, LSTM, Activation\n",
    "from keras import backend as K\n",
    "from keras.utils.generic_utils import get_custom_objects\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.regularizers import l1_l2\n",
    "\n",
    "dataset = pd.read_csv('C:/Users/Froilan/Desktop/myFiles/JupyterFiles/stock_correlation_prediction/train_dev_test/new_asset_after_arima.csv')\n",
    "dataset = dataset.loc[:,~dataset.columns.str.contains('^Unnamed')]\n",
    "X = dataset.loc[:,~dataset.columns.str.contains('20')]\n",
    "Y = dataset.loc[:,dataset.columns.str.contains('20')]\n",
    "\n",
    "X = np.asarray(X).reshape(180,20,1)\n",
    "Y = np.asarray(Y).reshape(180,1)\n",
    "\n",
    "\n",
    "#define custom activation\n",
    "class Double_Tanh(Activation):\n",
    "    def __init__(self, activation, **kwargs):\n",
    "        super(Double_Tanh, self).__init__(activation, **kwargs)\n",
    "        self.__name__ = 'double_tanh'\n",
    "\n",
    "def double_tanh(x):\n",
    "    return (K.tanh(x) * 2)\n",
    "\n",
    "get_custom_objects().update({'double_tanh':Double_Tanh(double_tanh)})\n",
    "\n",
    "\n",
    "\n",
    "model = load_model('C:/Users/Froilan/Desktop/myFiles/JupyterFiles/stock_correlation_prediction/models/hybrid_LSTM/epoch247.h5')\n",
    "score = model.evaluate(X,Y)\n",
    "print('score : mse - ' + str(np.round(score[1],4)) + ' / mae - ' + str(np.round(score[2], 4)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fbdd814",
   "metadata": {},
   "source": [
    "### new_asset_testing_beforeARIMA.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "591095b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from statsmodels.tsa.arima_model import ARIMA\n",
    "\n",
    "portfolio = ['CELG', 'PXD', 'WAT', 'LH', 'AMGN', 'AOS', 'EFX', 'CRM', 'NEM', 'JNPR', 'LB', 'CTAS', 'MAT', 'MDLZ', 'VLO', 'APH', 'ADM', 'MLM', 'BK', 'NOV', 'BDX', 'RRC', 'IVZ', 'ED', 'SBUX', 'GRMN', 'CI', 'ZION', 'COO', 'TIF', 'RHT', 'FDX', 'LLL', 'GLW', 'GPN', 'IPGP', 'GPC', 'HPQ', 'ADI', 'AMG', 'MTB', 'YUM', 'SYK', 'KMX', 'AME', 'AAP', 'DAL', 'A', 'MON', 'BRK', 'BMY', 'KMB', 'JPM', 'CCI', 'AET', 'DLTR', 'MGM', 'FL', 'HD', 'CLX', 'OKE', 'UPS', 'WMB', 'IFF', 'CMS', 'ARNC', 'VIAB', 'MMC', 'REG', 'ES', 'ITW', 'NDAQ', 'AIZ', 'VRTX', 'CTL', 'QCOM', 'MSI', 'NKTR', 'AMAT', 'BWA', 'ESRX', 'TXT', 'EXR', 'VNO', 'BBT', 'WDC', 'UAL', 'PVH', 'NOC', 'PCAR', 'NSC', 'UAA', 'FFIV', 'PHM', 'LUV', 'HUM', 'SPG', 'SJM', 'ABT', 'CMG', 'ALK', 'ULTA', 'TMK', 'TAP', 'SCG', 'CAT', 'TMO', 'AES', 'MRK', 'RMD', 'MKC', 'WU', 'ACN', 'HIG', 'TEL', 'DE', 'ATVI', 'O', 'UNM', 'VMC', 'ETFC', 'CMA', 'NRG', 'RHI', 'RE', 'FMC', 'MU', 'CB', 'LNT', 'GE', 'CBS', 'ALGN', 'SNA', 'LLY', 'LEN', 'MAA', 'OMC', 'F', 'APA', 'CDNS', 'SLG', 'HP', 'XLNX', 'SHW', 'AFL', 'STT', 'PAYX', 'AIG', 'FOX', 'MA']\n",
    "df = pd.read_csv(\"C:/Users/Froilan/Desktop/myFiles/JupyterFiles/stock_correlation_prediction/stock08_price.csv\")\n",
    "universe = list(df.columns.values[1:])\n",
    "universe.remove(\"SP500\")\n",
    "unselected_universe = list(set(universe)-set(portfolio))\n",
    "\n",
    "random.shuffle(unselected_universe)\n",
    "random.seed(1)\n",
    "new_assets = unselected_universe[:10].copy()\n",
    "print(new_assets)\n",
    "\n",
    "\n",
    "def rolling_corr(item1, item2):\n",
    "    # import data\n",
    "    stock_price_df = pd.read_csv(\"C:/Users/Froilan/Desktop/myFiles/JupyterFiles/stock_correlation_prediction/stock08_price.csv\")\n",
    "    pd.to_datetime(stock_price_df['Date'], format='%Y-%m-%d')\n",
    "    stock_price_df = stock_price_df.set_index(pd.DatetimeIndex(stock_price_df['Date']))\n",
    "\n",
    "    # calculate\n",
    "    df_pair = pd.concat([stock_price_df[item1], stock_price_df[item2]], axis=1)\n",
    "    df_pair.columns = [item1, item2]\n",
    "    df_corr = df_pair[item1].rolling(window=100).corr(df_pair[item2])\n",
    "    return df_corr\n",
    "\n",
    "\n",
    "data_matrix = []\n",
    "\n",
    "for i in range(len(new_assets)):\n",
    "    for j in range(len(new_assets)-1-i):\n",
    "        a = new_assets[i]\n",
    "        b = new_assets[9-j]\n",
    "        corr_series = rolling_corr(a,b)[99:]\n",
    "        corr_strided = list(corr_series[[100*k for k in range(24)]])\n",
    "        data_matrix.append(corr_strided)\n",
    "\n",
    "data_dictionary = {}\n",
    "for i in range(len(data_matrix)):\n",
    "    data_dictionary[str(i)] = data_matrix[i]\n",
    "data_df = pd.DataFrame(data_dictionary)\n",
    "\n",
    "before_arima_dataset = []\n",
    "for i in range(45):\n",
    "    before_arima_dataset.append(data_df[str(i)][:21])\n",
    "    before_arima_dataset.append(data_df[str(i)][1:22])\n",
    "    before_arima_dataset.append(data_df[str(i)][2:23])\n",
    "    before_arima_dataset.append(data_df[str(i)][3:])\n",
    "before_arima_dataset = pd.DataFrame(np.array(before_arima_dataset))\n",
    "before_arima_dataset.to_csv('C:/Users/Froilan/Desktop/myFiles/JupyterFiles/stock_correlation_prediction/train_dev_test/new_asset_before_arima.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89488aa9",
   "metadata": {},
   "source": [
    "### Residual_LSTM.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c46dbe4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import Dense, LSTM, Activation\n",
    "from keras import backend as K\n",
    "from keras.utils.generic_utils import get_custom_objects\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.regularizers import l1_l2\n",
    "\n",
    "\n",
    "# Train - Dev - Test Generation\n",
    "train_X= pd.read_csv('C:/Users/Froilan/Desktop/myFiles/JupyterFiles/stock_correlation_prediction/train_dev_test/after_arima/train_X.csv')\n",
    "print('loaded train_X')\n",
    "dev_X = pd.read_csv('C:/Users/Froilan/Desktop/myFiles/JupyterFiles/stock_correlation_prediction/train_dev_test/after_arima/dev_X.csv')\n",
    "print('loaded dev_X')\n",
    "test1_X = pd.read_csv('C:/Users/Froilan/Desktop/myFiles/JupyterFiles/stock_correlation_prediction/train_dev_test/after_arima/test1_X.csv')\n",
    "print('loaded test1_X')\n",
    "test2_X = pd.read_csv('C:/Users/Froilan/Desktop/myFiles/JupyterFiles/stock_correlation_prediction/train_dev_test/after_arima/test2_X.csv')\n",
    "print('loaded test2_X')\n",
    "train_Y = pd.read_csv('C:/Users/Froilan/Desktop/myFiles/JupyterFiles/stock_correlation_prediction/train_dev_test/after_arima/train_Y.csv')\n",
    "print('loaded train_Y')\n",
    "dev_Y = pd.read_csv('C:/Users/Froilan/Desktop/myFiles/JupyterFiles/stock_correlation_prediction/train_dev_test/after_arima/dev_Y.csv')\n",
    "print('loaded dev_Y')\n",
    "test1_Y = pd.read_csv('C:/Users/Froilan/Desktop/myFiles/JupyterFiles/stock_correlation_prediction/train_dev_test/after_arima/test1_Y.csv')\n",
    "print('loaded test1_Y')\n",
    "test2_Y = pd.read_csv('C:/Users/Froilan/Desktop/myFiles/JupyterFiles/stock_correlation_prediction/train_dev_test/after_arima/test2_Y.csv')\n",
    "print('loaded test2_Y')\n",
    "train_X = train_X.loc[:, ~train_X.columns.str.contains('^Unnamed')]\n",
    "dev_X = dev_X.loc[:, ~dev_X.columns.str.contains('^Unnamed')]\n",
    "test1_X = test1_X.loc[:, ~test1_X.columns.str.contains('^Unnamed')]\n",
    "test2_X = test2_X.loc[:, ~test2_X.columns.str.contains('^Unnamed')]\n",
    "train_Y = train_Y.loc[:, ~train_Y.columns.str.contains('^Unnamed')]\n",
    "dev_Y = dev_Y.loc[:, ~dev_Y.columns.str.contains('^Unnamed')]\n",
    "test1_Y = test1_Y.loc[:, ~test1_Y.columns.str.contains('^Unnamed')]\n",
    "test2_Y = test2_Y.loc[:, ~test2_Y.columns.str.contains('^Unnamed')]\n",
    "\n",
    "# data sampling\n",
    "STEP = 20\n",
    "#num_list = [STEP*i for i in range(int(1117500/STEP))]\n",
    "\n",
    "_train_X = np.asarray(train_X).reshape((int(1117500/STEP), 20, 1))\n",
    "_dev_X = np.asarray(dev_X).reshape((int(1117500/STEP), 20, 1))\n",
    "_test1_X = np.asarray(test1_X).reshape((int(1117500/STEP), 20, 1))\n",
    "_test2_X = np.asarray(test2_X).reshape((int(1117500/STEP), 20, 1))\n",
    "\n",
    "_train_Y = np.asarray(train_Y).reshape(int(1117500/STEP), 1)\n",
    "_dev_Y = np.asarray(dev_Y).reshape(int(1117500/STEP), 1)\n",
    "_test1_Y = np.asarray(test1_Y).reshape(int(1117500/STEP), 1)\n",
    "_test2_Y = np.asarray(test2_Y).reshape(int(1117500/STEP), 1)\n",
    "\n",
    "#define custom activation\n",
    "class Double_Tanh(Activation):\n",
    "    def __init__(self, activation, **kwargs):\n",
    "        super(Double_Tanh, self).__init__(activation, **kwargs)\n",
    "        self.__name__ = 'double_tanh'\n",
    "\n",
    "def double_tanh(x):\n",
    "    return (K.tanh(x) * 2)\n",
    "\n",
    "get_custom_objects().update({'double_tanh':Double_Tanh(double_tanh)})\n",
    "\n",
    "# Model Generation\n",
    "model = Sequential()\n",
    "#check https://machinelearningmastery.com/use-weight-regularization-lstm-networks-time-series-forecasting/\n",
    "model.add(LSTM(25, input_shape=(20,1), dropout=0.0, kernel_regularizer=l1_l2(0.00,0.00), bias_regularizer=l1_l2(0.00,0.00)))\n",
    "model.add(Dense(1))\n",
    "model.add(Activation(double_tanh))\n",
    "model.compile(loss='mean_squared_error', optimizer='adam', metrics=['mse', 'mae'])\n",
    "#, kernel_regularizer=l1_l2(0,0.1), bias_regularizer=l1_l2(0,0.1),\n",
    "\n",
    "print(model.metrics_names)\n",
    "# Fitting the Model\n",
    "model_scores = {}\n",
    "Reg = False\n",
    "d = 'hybrid_LSTM'\n",
    "\n",
    "if Reg :\n",
    "    d += '_with_reg'\n",
    "\n",
    "epoch_num=1\n",
    "for _ in range(124):\n",
    "\n",
    "    # train the model\n",
    "    dir = 'C:/Users/Froilan/Desktop/myFiles/JupyterFiles/stock_correlation_prediction/models/'+d\n",
    "    file_list = os.listdir(dir)\n",
    "    if len(file_list) != 0 :\n",
    "        epoch_num = len(file_list) + 1\n",
    "        recent_model_name = 'epoch'+str(epoch_num-1)+'.h5'\n",
    "        filepath = 'C:/Users/Froilan/Desktop/myFiles/JupyterFiles/stock_correlation_prediction/models/' + d + '/' + recent_model_name\n",
    "        model = load_model(filepath)\n",
    "\n",
    "    filepath = 'C:/Users/Froilan/Desktop/myFiles/JupyterFiles/stock_correlation_prediction/models/' + d + '/epoch'+str(epoch_num)+'.h5'\n",
    "\n",
    "    checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=False, mode='min')\n",
    "    callbacks_list = [checkpoint]\n",
    "    if len(callbacks_list) == 0:\n",
    "        model.fit(_train_X, _train_Y, epochs=1, batch_size=500, shuffle=True)\n",
    "    else:\n",
    "        model.fit(_train_X, _train_Y, epochs=1, batch_size=500, shuffle=True, callbacks=callbacks_list)\n",
    "\n",
    "    # test the model\n",
    "    score_train = model.evaluate(_train_X, _train_Y)\n",
    "    score_dev = model.evaluate(_dev_X, _dev_Y)\n",
    "    score_test1 = model.evaluate(_test1_X, _test1_Y)\n",
    "    score_test2 = model.evaluate(_test2_X, _test2_Y)\n",
    "\n",
    "    print('train set score : mse - ' + str(score_train[1]) +' / mae - ' + str(score_train[2]))\n",
    "    print('dev set score : mse - ' + str(score_dev[1]) +' / mae - ' + str(score_dev[2]))\n",
    "    print('test1 set score : mse - ' + str(score_test1[1]) +' / mae - ' + str(score_test1[2]))\n",
    "    print('test2 set score : mse - ' + str(score_test2[1]) +' / mae - ' + str(score_test2[2]))\n",
    "#.history['mean_squared_error'][0]\n",
    "    # get former score data\n",
    "    df = pd.read_csv(\"C:/Users/Froilan/Desktop/myFiles/JupyterFiles/stock_correlation_prediction/models/\"+d+\".csv\")\n",
    "    train_mse = list(df['TRAIN_MSE'])\n",
    "    dev_mse = list(df['DEV_MSE'])\n",
    "    test1_mse = list(df['TEST1_MSE'])\n",
    "    test2_mse = list(df['TEST2_MSE'])\n",
    "\n",
    "    train_mae = list(df['TRAIN_MAE'])\n",
    "    dev_mae = list(df['DEV_MAE'])\n",
    "    test1_mae = list(df['TEST1_MAE'])\n",
    "    test2_mae = list(df['TEST2_MAE'])\n",
    "\n",
    "    # append new data\n",
    "    train_mse.append(score_train[1])\n",
    "    dev_mse.append(score_dev[1])\n",
    "    test1_mse.append(score_test1[1])\n",
    "    test2_mse.append(score_test2[1])\n",
    "\n",
    "    train_mae.append(score_train[2])\n",
    "    dev_mae.append(score_dev[2])\n",
    "    test1_mae.append(score_test1[2])\n",
    "    test2_mae.append(score_test2[2])\n",
    "\n",
    "    # organize newly created score dataset\n",
    "    model_scores['TRAIN_MSE'] = train_mse\n",
    "    model_scores['DEV_MSE'] = dev_mse\n",
    "    model_scores['TEST1_MSE'] = test1_mse\n",
    "    model_scores['TEST2_MSE'] = test2_mse\n",
    "\n",
    "    model_scores['TRAIN_MAE'] = train_mae\n",
    "    model_scores['DEV_MAE'] = dev_mae\n",
    "    model_scores['TEST1_MAE'] = test1_mae\n",
    "    model_scores['TEST2_MAE'] = test2_mae\n",
    "\n",
    "    # save newly created score dataset\n",
    "    model_scores_df = pd.DataFrame(model_scores)\n",
    "    model_scores_df.to_csv(\"C:/Users/Froilan/Desktop/myFiles/JupyterFiles/stock_correlation_prediction/models/\"+d+\".csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "296ddf6c",
   "metadata": {},
   "source": [
    "## models\n",
    "\n",
    "The LSTM models saved for each epoch (models/hybrid_LSTM Folder) + The evaluated metric values for each epoch (models/hybrid_LSTM.csv)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a49c100a",
   "metadata": {},
   "source": [
    "## MODEL EVALUATOR\n",
    "\n",
    "The model performance is tested against other financial models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "911ddad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4185f82",
   "metadata": {},
   "source": [
    "### LSTM Model Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c9e3f59",
   "metadata": {},
   "outputs": [],
   "source": [
    "Reg = False\n",
    "ELBOW = 5\n",
    "d = 'hybrid_LSTM'\n",
    "\n",
    "if Reg :\n",
    "    d += '_with_reg'\n",
    "scores = pd.read_csv(\"C:/Users/Froilan/Desktop/myFiles/JupyterFiles/stock_correlation_prediction/models/\"+d+\".csv\")\n",
    "mse_columns = ['TRAIN_MSE','DEV_MSE']\n",
    "mae_columns = ['TRAIN_MAE','DEV_MAE']\n",
    "test_columns = ['TEST1_MSE', 'TEST2_MSE']\n",
    "\n",
    "#print(scores)\n",
    "\n",
    "end_epoch = scores.shape[0]\n",
    "print(end_epoch)\n",
    "plt.plot(scores[mse_columns[0]][:end_epoch],'--')\n",
    "plt.plot(scores[mse_columns[1]][:end_epoch])\n",
    "plt.legend()\n",
    "plt.xlabel('epochs')\n",
    "plt.ylabel('Mean Squared Error')\n",
    "plt.show()\n",
    "plt.close()\n",
    "plt.plot(scores[mae_columns[0]][:end_epoch],'--')\n",
    "plt.plot(scores[mae_columns[1]][:end_epoch])\n",
    "plt.legend()\n",
    "plt.xlabel('epochs')\n",
    "plt.ylabel('Mean Absolute Error')\n",
    "plt.show()\n",
    "plt.close()\n",
    "plt.plot(scores[test_columns[0]][:end_epoch],'--')\n",
    "plt.plot(scores[test_columns[1]][:end_epoch])\n",
    "plt.legend()\n",
    "plt.xlabel('epochs')\n",
    "plt.ylabel('Mean Squared Error')\n",
    "plt.show()\n",
    "plt.close()\n",
    "\n",
    "\n",
    "\n",
    "score_diff = (scores[mse_columns[1]]-scores[mse_columns[0]])[ELBOW:]\n",
    "\n",
    "score_sum = (scores[mse_columns[1]]+scores[mse_columns[0]])[ELBOW:]\n",
    "\n",
    "score_diff_norm = (score_diff - np.mean(score_diff))/np.std(score_diff)\n",
    "score_sum_norm = (score_sum - np.mean(score_sum))/np.std(score_sum)\n",
    "score_total = score_diff_norm + score_sum_norm\n",
    "idx = np.argmin(score_total)\n",
    "print('< OPT. SCORE_SUM EPOCH ',str(idx+1),'> : '+str(score_total[idx]))\n",
    "print('opt. DEV MSE : ',str(scores[mse_columns[1]][idx]))\n",
    "print('opt. TEST1 MSE : ',str(scores[test_columns[0]][idx]))\n",
    "print('opt. TEST2 MSE : ',str(scores[test_columns[1]][idx]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6a5088c",
   "metadata": {},
   "source": [
    "### Other Model Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6f3943a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dev = pd.read_csv(\"C:/Users/Froilan/Desktop/myFiles/JupyterFiles/stock_correlation_prediction/train_dev_test/before_arima/dev.csv\")\n",
    "#dev_Y = pd.read_csv(\"C:/Users/Froilan/Desktop/myFiles/JupyterFiles/stock_correlation_prediction/train_dev_test/after_arima/dev_Y.csv\")\n",
    "test1 = pd.read_csv(\"C:/Users/Froilan/Desktop/myFiles/JupyterFiles/stock_correlation_prediction/train_dev_test/before_arima/test1.csv\")\n",
    "#test1_Y = pd.read_csv(\"C:/Users/Froilan/Desktop/myFiles/JupyterFiles/stock_correlation_prediction/train_dev_test/after_arima/test1_Y.csv\")\n",
    "test2 = pd.read_csv(\"C:/Users/Froilan/Desktop/myFiles/JupyterFiles/stock_correlation_prediction/train_dev_test/before_arima/test2.csv\")\n",
    "#test2_Y = pd.read_csv(\"C:/Users/Froilan/Desktop/myFiles/JupyterFiles/stock_correlation_prediction/train_dev_test/after_arima/test2_Y.csv\")\n",
    "\n",
    "dev = dev.loc[:, ~dev.columns.str.contains('^Unnamed')]\n",
    "#dev_Y = dev_Y.loc[:, ~dev_Y.columns.str.contains('^Unnamed')]\n",
    "test1 = test1.loc[:, ~test1.columns.str.contains('^Unnamed')]\n",
    "#test1_Y = test1_Y.loc[:, ~test1_Y.columns.str.contains('^Unnamed')]\n",
    "test2 = test2.loc[:, ~test2.columns.str.contains('^Unnamed')]\n",
    "#test2_Y = test2_Y.loc[:, ~test2_Y.columns.str.contains('^Unnamed')]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96eacc76",
   "metadata": {},
   "source": [
    "#### Historical Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f36bb8d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "STEP = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75b7dbde",
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_pred = np.array(dev['20'])\n",
    "dev_y = np.array(dev['21']).reshape(1,int(1117500/STEP))[0]\n",
    "test1_pred = np.array(test1['21'])\n",
    "test1_y = np.array(test1['22']).reshape(1,int(1117500/STEP))[0]\n",
    "test2_pred = np.array(test2['22'])\n",
    "test2_y = np.array(test2['23']).reshape(1,int(1117500/STEP))[0]\n",
    "\n",
    "dev_mse = sum((dev_pred-dev_y)**2)/len(dev_pred)\n",
    "dev_mae = sum(abs(dev_pred-dev_y))/len(dev_pred)\n",
    "test1_mse = sum((test1_pred-test1_y)**2)/len(test1_pred)\n",
    "test1_mae = sum(abs(test1_pred-test1_y))/len(test1_pred)\n",
    "test2_mse = sum((test2_pred-test2_y)**2)/len(test2_pred)\n",
    "test2_mae = sum(abs(test2_pred-test2_y))/len(test2_pred)\n",
    "\n",
    "hist_matrix = [[dev_mse, dev_mae], [test1_mse, test1_mae], [test2_mse, test2_mae]]\n",
    "for i in hist_matrix :\n",
    "    print(str(i[0]) + '/' + str(i[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ec18bfc",
   "metadata": {},
   "source": [
    "#### Constant Correlation Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1a966f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = sum(dev['20'])/int(1117500/STEP)\n",
    "dev_pred = np.array([pred] * int(1117500/STEP))\n",
    "pred = sum(test1['21'])/int(1117500/STEP)\n",
    "test1_pred = np.array([pred] * int(1117500/STEP))\n",
    "pred = sum(test2['22'])/int(1117500/STEP)\n",
    "test2_pred = np.array([pred] * int(1117500/STEP))\n",
    "\n",
    "dev_mse = sum((dev_pred-dev_y)**2)/len(dev_pred)\n",
    "dev_mae = sum(abs(dev_pred-dev_y))/len(dev_pred)\n",
    "test1_mse = sum((test1_pred-test1_y)**2)/len(test1_pred)\n",
    "test1_mae = sum(abs(test1_pred-test1_y))/len(test1_pred)\n",
    "test2_mse = sum((test2_pred-test2_y)**2)/len(test2_pred)\n",
    "test2_mae = sum(abs(test2_pred-test2_y))/len(test2_pred)\n",
    "\n",
    "cc_matrix = [[dev_mse, dev_mae], [test1_mse, test1_mae], [test2_mse, test2_mae]]\n",
    "for i in cc_matrix :\n",
    "    print(str(i[0]) + '/' + str(i[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca38ec2f",
   "metadata": {},
   "source": [
    "#### Multi Group Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebf5baf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df = pd.read_csv('C:/Users/Froilan/Desktop/myFiles/JupyterFiles/stock_correlation_prediction/dataset.csv')\n",
    "data_df = data_df.loc[:, ~data_df.columns.str.contains('^Unnamed')]\n",
    "num_list = []\n",
    "for i in range(24):\n",
    "    num_list.append(str(i))\n",
    "data_df = data_df[num_list].copy()\n",
    "data_df = np.transpose(data_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9552dbe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('C:/Users/Froilan/Desktop/myFiles/JupyterFiles/stock_correlation_prediction/SP500_list.csv')\n",
    "print(data['GICS_sector'].unique())\n",
    "print(data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b424b6e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# portfolio list\n",
    "portfolio = ['CELG', 'PXD', 'WAT', 'LH', 'AMGN', 'AOS', 'EFX', 'CRM', 'NEM', 'JNPR', 'LB', 'CTAS', 'MAT', 'MDLZ', 'VLO', 'APH', 'ADM', 'MLM', 'BK', 'NOV', 'BDX', 'RRC', 'IVZ', 'ED', 'SBUX', 'GRMN', 'CI', 'ZION', 'COO', 'TIF', 'RHT', 'FDX', 'LLL', 'GLW', 'GPN', 'IPGP', 'GPC', 'HPQ', 'ADI', 'AMG', 'MTB', 'YUM', 'SYK', 'KMX', 'AME', 'AAP', 'DAL', 'A', 'MON', 'BRK', 'BMY', 'KMB', 'JPM', 'CCI', 'AET', 'DLTR', 'MGM', 'FL', 'HD', 'CLX', 'OKE', 'UPS', 'WMB', 'IFF', 'CMS', 'ARNC', 'VIAB', 'MMC', 'REG', 'ES', 'ITW', 'NDAQ', 'AIZ', 'VRTX', 'CTL', 'QCOM', 'MSI', 'NKTR', 'AMAT', 'BWA', 'ESRX', 'TXT', 'EXR', 'VNO', 'BBT', 'WDC', 'UAL', 'PVH', 'NOC', 'PCAR', 'NSC', 'UAA', 'FFIV', 'PHM', 'LUV', 'HUM', 'SPG', 'SJM', 'ABT', 'CMG', 'ALK', 'ULTA', 'TMK', 'TAP', 'SCG', 'CAT', 'TMO', 'AES', 'MRK', 'RMD', 'MKC', 'WU', 'ACN', 'HIG', 'TEL', 'DE', 'ATVI', 'O', 'UNM', 'VMC', 'ETFC', 'CMA', 'NRG', 'RHI', 'RE', 'FMC', 'MU', 'CB', 'LNT', 'GE', 'CBS', 'ALGN', 'SNA', 'LLY', 'LEN', 'MAA', 'OMC', 'F', 'APA', 'CDNS', 'SLG', 'HP', 'XLNX', 'SHW', 'AFL', 'STT', 'PAYX', 'AIG', 'FOX', 'MA']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c42964dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "pf_sector_item = {'Industrials':[],\n",
    "                  'Health Care':[],\n",
    "                  'Information Technology':[],\n",
    "                  'Consumer Discretionary':[],\n",
    "                  'Utilities':[],\n",
    "                  'Financials' :[],\n",
    "                  'Materials':[],\n",
    "                  'Real Estate':[],\n",
    "                  'Consumer Staples':[],\n",
    "                  'Energy':[],\n",
    "                  'Telecommunication Services':[]}\n",
    "for item in portfolio :\n",
    "    pf_sector_item[data[data.ticker == item]['GICS_sector'].values[0]] = pf_sector_item[data[data.ticker == item]['GICS_sector'].values[0]]+[item]\n",
    "print(pf_sector_item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b9f252d",
   "metadata": {},
   "outputs": [],
   "source": [
    "market_data = pd.read_csv('C:/Users/Froilan/Desktop/myFiles/JupyterFiles/stock_correlation_prediction/stock08_price.csv')\n",
    "pf_sector_dev = {}\n",
    "pf_sector_test1 = {}\n",
    "pf_sector_test2 = {}\n",
    "\n",
    "for i in range(150):\n",
    "    for j in range(149-i):\n",
    "        a = portfolio[i]\n",
    "        b = portfolio[149-j]\n",
    "        a_price = market_data[a]\n",
    "        b_price = market_data[b]\n",
    "        a_sector = data[data.ticker == a]['GICS_sector'].values[0]\n",
    "        b_sector = data[data.ticker == b]['GICS_sector'].values[0]\n",
    "        sector_pair = max(a_sector, b_sector)+'_'+min(a_sector, b_sector)\n",
    "        \n",
    "        dev = []\n",
    "        test1 = []\n",
    "        test2 = []\n",
    "        for k in range(5):\n",
    "            dev_start = 2000 + k*20\n",
    "            test1_start = 2100 + k*20\n",
    "            test2_start = 2200 + k*20\n",
    "            dev.append(a_price[dev_start:dev_start+100].corr(b_price[dev_start:dev_start+100]))\n",
    "            test1.append(a_price[test1_start:test1_start+100].corr(b_price[test1_start:test1_start+100]))\n",
    "            test2.append(a_price[test2_start:test2_start+100].corr(b_price[test2_start:test2_start+100]))\n",
    "        \n",
    "        try:\n",
    "            pf_sector_dev[sector_pair] = pf_sector_dev[sector_pair] + [dev]\n",
    "        except KeyError :\n",
    "            pf_sector_dev[sector_pair] = [dev]\n",
    "            \n",
    "        try:\n",
    "            pf_sector_test1[sector_pair] = pf_sector_test1[sector_pair] + [test1]\n",
    "        except KeyError :\n",
    "            pf_sector_test1[sector_pair] = [test1]\n",
    "            \n",
    "        try:\n",
    "            pf_sector_test2[sector_pair] = pf_sector_test2[sector_pair] + [test2]\n",
    "        except KeyError :\n",
    "            pf_sector_test2[sector_pair] = [test2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a07efe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "pairs = [key for key in pf_sector_dev]\n",
    "sector_pair_corr_dev = {}\n",
    "sector_pair_corr_test1 = {}\n",
    "sector_pair_corr_test2 = {}\n",
    "for pair in pairs :\n",
    "    dev_zeroes = np.array([0] * 5)\n",
    "    test1_zeroes = np.array([0] * 5)\n",
    "    test2_zeroes = np.array([0] * 5)\n",
    "    dev_length = len(pf_sector_dev[pair])\n",
    "    test1_length = len(pf_sector_test1[pair])\n",
    "    test2_length = len(pf_sector_test2[pair])\n",
    "    for arr in pf_sector_dev[pair] :\n",
    "        dev_zeroes = dev_zeroes + np.array(arr)\n",
    "        dev_result = dev_zeroes/dev_length\n",
    "    for arr in pf_sector_test1[pair] :\n",
    "        test1_zeroes = test1_zeroes + np.array(arr)\n",
    "        test1_result = test1_zeroes/test1_length\n",
    "    for arr in pf_sector_test2[pair] :\n",
    "        test2_zeroes = test2_zeroes + np.array(arr)\n",
    "        test2_result = test2_zeroes/test2_length\n",
    "    sector_pair_corr_dev[pair] = dev_result\n",
    "    sector_pair_corr_test1[pair] = test1_result\n",
    "    sector_pair_corr_test2[pair] = test2_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ee28411",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_list = [STEP*i for i in range(int(1117500/STEP))]\n",
    "dataset = pd.read_csv(\"C:/Users/Froilan/Desktop/myFiles/JupyterFiles/stock_correlation_prediction/dataset.csv\")\n",
    "dev_y = dataset['21'].copy()\n",
    "test1_y = dataset['22'].copy()\n",
    "test2_y = dataset['23'].copy()\n",
    "\n",
    "dev_y = np.array(dev_y[num_list]).reshape(1,int(1117500/STEP))[0]\n",
    "test1_y = np.array(test1_y[num_list]).reshape(1,int(1117500/STEP))[0]\n",
    "test2_y = np.array(test2_y[num_list]).reshape(1,int(1117500/STEP))[0]\n",
    "\n",
    "dev_pred = []\n",
    "test1_pred = []\n",
    "test2_pred = []\n",
    "for i in range(150):\n",
    "    for j in range(149-i):\n",
    "        a = portfolio[i]\n",
    "        b = portfolio[149-j]\n",
    "        a_sector = data[data.ticker == a]['GICS_sector'].values[0]\n",
    "        b_sector = data[data.ticker == b]['GICS_sector'].values[0]\n",
    "        sector_pair = max(a_sector, b_sector)+'_'+min(a_sector, b_sector)\n",
    "        \n",
    "        dev_pred = dev_pred + list(sector_pair_corr_dev[sector_pair])\n",
    "        test1_pred = test1_pred + list(sector_pair_corr_test1[sector_pair])\n",
    "        test2_pred = test2_pred + list(sector_pair_corr_test2[sector_pair])\n",
    "dev_pred = np.array(dev_pred)\n",
    "test1_pred = np.array(test1_pred)\n",
    "test2_pred = np.array(test2_pred)\n",
    "\n",
    "\n",
    "dev_mse = sum((dev_pred-dev_y)**2)/len(dev_pred)\n",
    "dev_mae = sum(abs(dev_pred-dev_y))/len(dev_pred)\n",
    "test1_mse = sum((test1_pred-test1_y)**2)/len(test1_pred)\n",
    "test1_mae = sum(abs(test1_pred-test1_y))/len(test1_pred)\n",
    "test2_mse = sum((test2_pred-test2_y)**2)/len(test2_pred)\n",
    "test2_mae = sum(abs(test2_pred-test2_y))/len(test2_pred)\n",
    "\n",
    "mg_matrix = [[dev_mse, dev_mae], [test1_mse, test1_mae], [test2_mse, test2_mae]]\n",
    "for i in mg_matrix :\n",
    "    print(str(i[0]) + '/' + str(i[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c54e1973",
   "metadata": {},
   "source": [
    "#### Single Index Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3104668",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df = pd.read_csv('C:/Users/Froilan/Desktop/myFiles/JupyterFiles/stock_correlation_prediction/stock08_price.csv')\n",
    "data_df = data_df.loc[:, ~data_df.columns.str.contains('^Unnamed')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "200535ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_pred = []\n",
    "test1_pred = []\n",
    "test2_pred = []\n",
    "\n",
    "for i in range(150):\n",
    "    for j in range(149-i):\n",
    "        a = portfolio[i]\n",
    "        b = portfolio[149-j]\n",
    "        for k in range(5):\n",
    "            dev_start = 2000 + k*20\n",
    "            test1_start = 2100 + k*20\n",
    "            test2_start = 2200 + k*20\n",
    "            dev_pred.append(data_df[a][dev_start:dev_start+100].corr(data_df['SP500'][dev_start:dev_start+100]) *\n",
    "                            data_df[b][dev_start:dev_start+100].corr(data_df['SP500'][dev_start:dev_start+100]))\n",
    "            test1_pred.append(data_df[a][test1_start:test1_start+100].corr(data_df['SP500'][test1_start:test1_start+100])*\n",
    "                              data_df[b][test1_start:test1_start+100].corr(data_df['SP500'][test1_start:test1_start+100]))\n",
    "            test2_pred.append(data_df[a][test2_start:test2_start+100].corr(data_df['SP500'][test2_start:test2_start+100])*\n",
    "                              data_df[b][test2_start:test2_start+100].corr(data_df['SP500'][test2_start:test2_start+100]))\n",
    "dev_pred = np.array(dev_pred)\n",
    "test1_pred = np.array(test1_pred)\n",
    "test2_pred = np.array(test2_pred)\n",
    "            \n",
    "num_list = [STEP*i for i in range(int(1117500/STEP))]\n",
    "dataset = pd.read_csv(\"C:/Users/Froilan/Desktop/myFiles/JupyterFiles/stock_correlation_prediction/dataset.csv\")\n",
    "dev_y = dataset['21'].copy()\n",
    "test1_y = dataset['22'].copy()\n",
    "test2_y = dataset['23'].copy()\n",
    "\n",
    "dev_y = np.array(dev_y[num_list]).reshape(1,int(1117500/STEP))[0]\n",
    "test1_y = np.array(test1_y[num_list]).reshape(1,int(1117500/STEP))[0]\n",
    "test2_y = np.array(test2_y[num_list]).reshape(1,int(1117500/STEP))[0]\n",
    "\n",
    "  \n",
    "\n",
    "dev_mse = sum((dev_pred-dev_y)**2)/len(dev_pred)\n",
    "dev_mae = sum(abs(dev_pred-dev_y))/len(dev_pred)\n",
    "test1_mse = sum((test1_pred-test1_y)**2)/len(test1_pred)\n",
    "test1_mae = sum(abs(test1_pred-test1_y))/len(test1_pred)\n",
    "test2_mse = sum((test2_pred-test2_y)**2)/len(test2_pred)\n",
    "test2_mae = sum(abs(test2_pred-test2_y))/len(test2_pred)\n",
    "\n",
    "mg_matrix = [[dev_mse, dev_mae], [test1_mse, test1_mae], [test2_mse, test2_mae]]\n",
    "for i in mg_matrix :\n",
    "    print(str(i[0]) + '/' + str(i[1])) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9f65928",
   "metadata": {},
   "source": [
    "## MISCELLANEOUS\n",
    "\n",
    "Literally... MISCELLANEOUS!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "914af02b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "586f898f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('C:/Users/Froilan/Desktop/myFiles/JupyterFiles/stock_correlation_prediction/train_dev_test/before_arima/train.csv')\n",
    "data = np.transpose(data.loc[:,~data.columns.str.contains(\"^Unnamed\")])\n",
    "print(data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5003dbb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "data[0].plot()\n",
    "plt.xlabel('time step')\n",
    "plt.ylabel('correlation coefficient')\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39d3187a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
